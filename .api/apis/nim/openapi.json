{
  "openapi": "3.1.0",
  "info": {
    "title": "NVIDIA NIM API for nvidia/llama-3.1-nemotron-70b-instruct",
    "description": "The NVIDIA NIM REST API. Please see https://docs.api.nvidia.com/nim/reference/nvidia-llama-3_1-nemotron-70b-instruct for more details.",
    "version": "1.0.0",
    "termsOfService": "https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/",
    "contact": {
      "name": "NVIDIA Enterprise Support",
      "url": "https://www.nvidia.com/en-us/support/enterprise/"
    },
    "license": {
      "name": "Llama 3.1 License",
      "url": "https://github.com/meta-llama/llama-models/blob/main/License/Llama3.1.txt"
    }
  },
  "servers": [
    {
      "url": "https://integrate.api.nvidia.com/v1/"
    }
  ],
  "paths": {
    "/chat/completions": {
      "post": {
        "operationId": "create_chat_completion_v1_chat_completions_post",
        "tags": [
          "Chat"
        ],
        "summary": "Creates a model response for the given chat conversation.",
        "description": "Given a list of messages comprising a conversation, the model will return a response. Compatible with OpenAI. See https://platform.openai.com/docs/api-reference/chat/create",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "properties": {
                  "model": {
                    "type": "string",
                    "title": "Model",
                    "default": "nvidia/llama-3.1-nemotron-70b-instruct"
                  },
                  "max_tokens": {
                    "type": "integer",
                    "minimum": 1,
                    "title": "Max Tokens",
                    "description": "The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.",
                    "default": 1024
                  },
                  "stream": {
                    "type": "boolean",
                    "title": "Stream",
                    "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by `data: `), with the stream terminated by a `data: [DONE]` message.",
                    "default": false
                  },
                  "temperature": {
                    "type": "number",
                    "maximum": 1,
                    "minimum": 0,
                    "title": "Temperature",
                    "description": "The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.",
                    "default": 0.5
                  },
                  "top_p": {
                    "type": "number",
                    "maximum": 1,
                    "exclusiveMinimum": 0,
                    "title": "Top P",
                    "description": "The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.",
                    "default": 1
                  },
                  "stop": {
                    "anyOf": [
                      {
                        "items": {
                          "type": "string"
                        },
                        "type": "array"
                      },
                      {
                        "type": "string"
                      },
                      {
                        "type": "null"
                      }
                    ],
                    "title": "Stop",
                    "description": "A string or a list of strings where the API will stop generating further tokens. The returned text will not contain the stop sequence.",
                    "examples": [
                      null
                    ]
                  },
                  "frequency_penalty": {
                    "type": "number",
                    "maximum": 2,
                    "minimum": -2,
                    "default": 0,
                    "title": "Frequency Penalty",
                    "description": "Indicates how much to penalize new tokens based on their existing frequency in the text so far, decreasing model likelihood to repeat the same line verbatim."
                  },
                  "presence_penalty": {
                    "type": "number",
                    "maximum": 2,
                    "minimum": -2,
                    "default": 0,
                    "title": "Presence Penalty",
                    "description": "Positive values penalize new tokens based on whether they appear in the text so far, increasing model likelihood to talk about new topics."
                  },
                  "seed": {
                    "type": "integer",
                    "maximum": 18446744073709552000,
                    "minimum": 0,
                    "title": "Seed",
                    "description": "The model generates random results. Changing the input seed alone will produce a different response with similar characteristics. It is possible to reproduce results by fixing the input seed (assuming all other hyperparameters are also fixed).",
                    "default": 0
                  },
                  "messages": {
                    "anyOf": [
                      {
                        "items": {
                          "additionalProperties": {
                            "type": "string"
                          },
                          "type": "object"
                        },
                        "type": "array"
                      }
                    ],
                    "title": "Messages",
                    "description": "A list of messages comprising the conversation so far.",
                    "examples": [
                      [
                        {
                          "role": "user",
                          "content": "Write a limerick about the wonders of GPU computing."
                        }
                      ]
                    ]
                  }
                },
                "additionalProperties": false,
                "type": "object",
                "required": [
                  "messages"
                ],
                "title": "ChatCompletionRequest",
                "description": "OpenAI ChatCompletionRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "properties": {
                    "id": {
                      "type": "string",
                      "title": "Id",
                      "description": "A unique identifier for the completion."
                    },
                    "object": {
                      "type": "string",
                      "title": "Object",
                      "default": "chat.completion"
                    },
                    "created": {
                      "type": "integer",
                      "title": "Created"
                    },
                    "model": {
                      "type": "string",
                      "title": "Model",
                      "example": "nvidia/llama-3.1-nemotron-70b-instruct"
                    },
                    "choices": {
                      "items": {
                        "properties": {
                          "index": {
                            "type": "integer",
                            "title": "Index",
                            "description": "The index of the choice in the list of choices (always 0)."
                          },
                          "message": {
                            "description": "A chat completion message generated by the model.",
                            "properties": {
                              "role": {
                                "type": "string",
                                "title": "Role",
                                "description": "The role of the message author."
                              },
                              "content": {
                                "type": "string",
                                "title": "Content",
                                "description": "The contents of the message."
                              }
                            },
                            "type": "object",
                            "required": [
                              "role",
                              "content"
                            ],
                            "title": "ChatMessage"
                          },
                          "finish_reason": {
                            "anyOf": [
                              {
                                "type": "string",
                                "enum": [
                                  "stop",
                                  "length"
                                ]
                              },
                              {
                                "type": "null"
                              }
                            ],
                            "title": "Finish Reason",
                            "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, or `length` if the maximum number of tokens specified in the request was reached. Will be `null` if the model has not finished"
                          }
                        },
                        "type": "object",
                        "required": [
                          "index",
                          "message"
                        ],
                        "title": "ChatCompletionResponseChoice"
                      },
                      "type": "array",
                      "title": "Choices",
                      "description": "The list of completion choices the model generated for the input prompt."
                    },
                    "usage": {
                      "description": "Usage statistics for the completion request.",
                      "properties": {
                        "prompt_tokens": {
                          "type": "integer",
                          "title": "Prompt Tokens",
                          "description": "Number of tokens in the prompt.",
                          "default": 0
                        },
                        "total_tokens": {
                          "type": "integer",
                          "title": "Total Tokens",
                          "description": "Total number of tokens used in the request (prompt + completion).",
                          "default": 0
                        },
                        "completion_tokens": {
                          "anyOf": [
                            {
                              "type": "integer"
                            },
                            {
                              "type": "null"
                            }
                          ],
                          "title": "Completion Tokens",
                          "description": "Number of tokens in the generated completion.",
                          "default": 0
                        }
                      },
                      "type": "object",
                      "title": "UsageInfo"
                    }
                  },
                  "type": "object",
                  "required": [
                    "model",
                    "choices",
                    "usage"
                  ],
                  "title": "ChatCompletionResponse"
                }
              }
            }
          },
          "402": {
            "description": "Payment Required",
            "content": {
              "application/json": {
                "schema": {
                  "properties": {
                    "detail": {
                      "type": "string",
                      "description": "Contains specific information related to the error and why it occurred.",
                      "example": "You have reached your limit of credits."
                    }
                  },
                  "type": "object",
                  "title": "PaymentRequiredError"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "properties": {
                    "detail": {
                      "items": {
                        "properties": {
                          "loc": {
                            "items": {
                              "anyOf": [
                                {
                                  "type": "string"
                                },
                                {
                                  "type": "integer"
                                }
                              ]
                            },
                            "type": "array",
                            "title": "Location"
                          },
                          "msg": {
                            "type": "string",
                            "title": "Message",
                            "description": "The error message."
                          },
                          "type": {
                            "type": "string",
                            "title": "Error Type",
                            "description": "Error type"
                          }
                        },
                        "type": "object",
                        "required": [
                          "loc",
                          "msg",
                          "type"
                        ],
                        "title": "ValidationError"
                      },
                      "type": "array",
                      "title": "Detail",
                      "description": "Detailed information about the error."
                    }
                  },
                  "type": "object",
                  "title": "HTTPValidationError"
                }
              }
            }
          }
        },
        "x-nvai-meta": {
          "name": "Create chat completion",
          "returns": "Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.\n",
          "path": "create",
          "examples": [
            {
              "name": "Write a limerick about the wonders of GPU computing.",
              "requestJson": "{\n      \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Write a limerick about the wonders of GPU computing.\"\n        }\n      ],\n      \"top_p\": 0.7,\n      \"max_tokens\": 1024,\n      \"seed\": 42,\n      \"stop\": null,\n      \"stream\": true\n}\n",
              "responseJson": "{\n  \"id\": \"id-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Here's a short poem on...\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21\n  }\n}\n"
            },
            {
              "name": "What can I see at NVIDIA's GPU Technology Conference?",
              "requestJson": "{\n      \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful assistant.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What can I see at NVIDIA's GPU Technology Conference?\"\n        }\n      ],\n      \"top_p\": 0.7,\n      \"max_tokens\": 1024,\n      \"seed\": 42,\n      \"stop\": null,\n      \"stream\": true\n}\n",
              "responseJson": "{\n  \"id\": \"id-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"At NVIDIA's GTC conference...\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21\n  }\n}\n"
            }
          ],
          "templates": [
            {
              "title": "No Streaming",
              "requestEjs": {
                "python": "from openai import OpenAI\n\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = \"$NVIDIA_API_KEY\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"<%- request.model %>\",\n  messages=<%- JSON.stringify(request.messages) %>,\n  temperature=<%- request.temperature %>,\n  top_p=<%- request.top_p %>,\n  max_tokens=<%- request.max_tokens %>,\n  stream=<%- request.stream?.toString()[0].toUpperCase() + request.stream?.toString().slice(1) %>\n)\n<% if (request.stream) { %>\nfor chunk in completion:\n  if chunk.choices[0].delta.content is not None:\n    print(chunk.choices[0].delta.content, end=\"\")\n<% } else { %>\nprint(completion.choices[0].message)\n<% } %>\n",
                "node.js": "import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: '$NVIDIA_API_KEY',\n  baseURL: 'https://integrate.api.nvidia.com/v1',\n})\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: \"<%- request.model %>\",\n    messages: <%- JSON.stringify(request.messages) %>,\n    temperature: <%- request.temperature %>,\n    top_p: <%- request.top_p %>,\n    max_tokens: <%- request.max_tokens %>,\n    stream: <%- request.stream %>,\n  })\n   <% if (request.stream) { %>\n  for await (const chunk of completion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '')\n  }\n  <% } else { %>\n  process.stdout.write(completion.choices[0]?.message?.content);\n  <% } %>\n}\n\nmain();",
                "curl": "curl https://integrate.api.nvidia.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $NVIDIA_API_KEY\" \\\n  -d '{\n    \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n    \"messages\": <%- JSON.stringify(request.messages).replaceAll(\"'\", \"'\\\"'\\\"'\") %>,\n    \"temperature\": <%- request.temperature %>,   \n    \"top_p\": <%- request.top_p %>,\n    \"max_tokens\": <%- request.max_tokens %>,\n    \"stream\": <%- request.stream %>                \n  }'\n"
              },
              "response": "{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"nvidia/llama-3.1-nemotron-70b-instruct\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21\n  }\n}\n"
            }
          ]
        }
      }
    }
  },
  "security": [
    {
      "Token": []
    }
  ],
  "components": {
    "securitySchemes": {
      "Token": {
        "type": "http",
        "scheme": "bearer"
      }
    },
    "schemas": {
      "ChatCompletionRequest": {
        "properties": {
          "model": {
            "type": "string",
            "title": "Model",
            "default": "nvidia/llama-3.1-nemotron-70b-instruct"
          },
          "max_tokens": {
            "type": "integer",
            "minimum": 1,
            "title": "Max Tokens",
            "description": "The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.",
            "default": 1024
          },
          "stream": {
            "type": "boolean",
            "title": "Stream",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by `data: `), with the stream terminated by a `data: [DONE]` message.",
            "default": false
          },
          "temperature": {
            "type": "number",
            "maximum": 1,
            "minimum": 0,
            "title": "Temperature",
            "description": "The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.",
            "default": 0.5
          },
          "top_p": {
            "type": "number",
            "maximum": 1,
            "exclusiveMinimum": 0,
            "title": "Top P",
            "description": "The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.",
            "default": 1
          },
          "stop": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Stop",
            "description": "A string or a list of strings where the API will stop generating further tokens. The returned text will not contain the stop sequence.",
            "examples": [
              null
            ]
          },
          "frequency_penalty": {
            "type": "number",
            "maximum": 2,
            "minimum": -2,
            "default": 0,
            "title": "Frequency Penalty",
            "description": "Indicates how much to penalize new tokens based on their existing frequency in the text so far, decreasing model likelihood to repeat the same line verbatim."
          },
          "presence_penalty": {
            "type": "number",
            "maximum": 2,
            "minimum": -2,
            "default": 0,
            "title": "Presence Penalty",
            "description": "Positive values penalize new tokens based on whether they appear in the text so far, increasing model likelihood to talk about new topics."
          },
          "seed": {
            "type": "integer",
            "maximum": 18446744073709552000,
            "minimum": 0,
            "title": "Seed",
            "description": "The model generates random results. Changing the input seed alone will produce a different response with similar characteristics. It is possible to reproduce results by fixing the input seed (assuming all other hyperparameters are also fixed).",
            "default": 0
          },
          "messages": {
            "anyOf": [
              {
                "items": {
                  "additionalProperties": {
                    "type": "string"
                  },
                  "type": "object"
                },
                "type": "array"
              }
            ],
            "title": "Messages",
            "description": "A list of messages comprising the conversation so far.",
            "examples": [
              [
                {
                  "role": "user",
                  "content": "Write a limerick about the wonders of GPU computing."
                }
              ]
            ]
          }
        },
        "additionalProperties": false,
        "type": "object",
        "required": [
          "messages"
        ],
        "title": "ChatCompletionRequest",
        "description": "OpenAI ChatCompletionRequest"
      },
      "ChatCompletionResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "A unique identifier for the completion."
          },
          "object": {
            "type": "string",
            "title": "Object",
            "default": "chat.completion"
          },
          "created": {
            "type": "integer",
            "title": "Created"
          },
          "model": {
            "type": "string",
            "title": "Model",
            "example": "nvidia/llama-3.1-nemotron-70b-instruct"
          },
          "choices": {
            "items": {
              "properties": {
                "index": {
                  "type": "integer",
                  "title": "Index",
                  "description": "The index of the choice in the list of choices (always 0)."
                },
                "message": {
                  "description": "A chat completion message generated by the model.",
                  "properties": {
                    "role": {
                      "type": "string",
                      "title": "Role",
                      "description": "The role of the message author."
                    },
                    "content": {
                      "type": "string",
                      "title": "Content",
                      "description": "The contents of the message."
                    }
                  },
                  "type": "object",
                  "required": [
                    "role",
                    "content"
                  ],
                  "title": "ChatMessage"
                },
                "finish_reason": {
                  "anyOf": [
                    {
                      "type": "string",
                      "enum": [
                        "stop",
                        "length"
                      ]
                    },
                    {
                      "type": "null"
                    }
                  ],
                  "title": "Finish Reason",
                  "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, or `length` if the maximum number of tokens specified in the request was reached. Will be `null` if the model has not finished"
                }
              },
              "type": "object",
              "required": [
                "index",
                "message"
              ],
              "title": "ChatCompletionResponseChoice"
            },
            "type": "array",
            "title": "Choices",
            "description": "The list of completion choices the model generated for the input prompt."
          },
          "usage": {
            "description": "Usage statistics for the completion request.",
            "properties": {
              "prompt_tokens": {
                "type": "integer",
                "title": "Prompt Tokens",
                "description": "Number of tokens in the prompt.",
                "default": 0
              },
              "total_tokens": {
                "type": "integer",
                "title": "Total Tokens",
                "description": "Total number of tokens used in the request (prompt + completion).",
                "default": 0
              },
              "completion_tokens": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "null"
                  }
                ],
                "title": "Completion Tokens",
                "description": "Number of tokens in the generated completion.",
                "default": 0
              }
            },
            "type": "object",
            "title": "UsageInfo"
          }
        },
        "type": "object",
        "required": [
          "model",
          "choices",
          "usage"
        ],
        "title": "ChatCompletionResponse"
      },
      "ChatCompletionResponseChoice": {
        "properties": {
          "index": {
            "type": "integer",
            "title": "Index",
            "description": "The index of the choice in the list of choices (always 0)."
          },
          "message": {
            "description": "A chat completion message generated by the model.",
            "properties": {
              "role": {
                "type": "string",
                "title": "Role",
                "description": "The role of the message author."
              },
              "content": {
                "type": "string",
                "title": "Content",
                "description": "The contents of the message."
              }
            },
            "type": "object",
            "required": [
              "role",
              "content"
            ],
            "title": "ChatMessage"
          },
          "finish_reason": {
            "anyOf": [
              {
                "type": "string",
                "enum": [
                  "stop",
                  "length"
                ]
              },
              {
                "type": "null"
              }
            ],
            "title": "Finish Reason",
            "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, or `length` if the maximum number of tokens specified in the request was reached. Will be `null` if the model has not finished"
          }
        },
        "type": "object",
        "required": [
          "index",
          "message"
        ],
        "title": "ChatCompletionResponseChoice"
      },
      "ChatMessage": {
        "properties": {
          "role": {
            "type": "string",
            "title": "Role",
            "description": "The role of the message author."
          },
          "content": {
            "type": "string",
            "title": "Content",
            "description": "The contents of the message."
          }
        },
        "type": "object",
        "required": [
          "role",
          "content"
        ],
        "title": "ChatMessage"
      },
      "HTTPValidationError": {
        "properties": {
          "detail": {
            "items": {
              "properties": {
                "loc": {
                  "items": {
                    "anyOf": [
                      {
                        "type": "string"
                      },
                      {
                        "type": "integer"
                      }
                    ]
                  },
                  "type": "array",
                  "title": "Location"
                },
                "msg": {
                  "type": "string",
                  "title": "Message",
                  "description": "The error message."
                },
                "type": {
                  "type": "string",
                  "title": "Error Type",
                  "description": "Error type"
                }
              },
              "type": "object",
              "required": [
                "loc",
                "msg",
                "type"
              ],
              "title": "ValidationError"
            },
            "type": "array",
            "title": "Detail",
            "description": "Detailed information about the error."
          }
        },
        "type": "object",
        "title": "HTTPValidationError"
      },
      "PaymentRequiredError": {
        "properties": {
          "detail": {
            "type": "string",
            "description": "Contains specific information related to the error and why it occurred.",
            "example": "You have reached your limit of credits."
          }
        },
        "type": "object",
        "title": "PaymentRequiredError"
      },
      "UsageInfo": {
        "properties": {
          "prompt_tokens": {
            "type": "integer",
            "title": "Prompt Tokens",
            "description": "Number of tokens in the prompt.",
            "default": 0
          },
          "total_tokens": {
            "type": "integer",
            "title": "Total Tokens",
            "description": "Total number of tokens used in the request (prompt + completion).",
            "default": 0
          },
          "completion_tokens": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "title": "Completion Tokens",
            "description": "Number of tokens in the generated completion.",
            "default": 0
          }
        },
        "type": "object",
        "title": "UsageInfo"
      },
      "ValidationError": {
        "properties": {
          "loc": {
            "items": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "integer"
                }
              ]
            },
            "type": "array",
            "title": "Location"
          },
          "msg": {
            "type": "string",
            "title": "Message",
            "description": "The error message."
          },
          "type": {
            "type": "string",
            "title": "Error Type",
            "description": "Error type"
          }
        },
        "type": "object",
        "required": [
          "loc",
          "msg",
          "type"
        ],
        "title": "ValidationError"
      }
    }
  }
}